{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90566,"databundleVersionId":11498594,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Read Excel files\nsolutions_df = pd.read_excel(\"/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAINING_SOLUTIONS.xlsx\")\ncategorical_df = pd.read_excel(\"/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_CATEGORICAL_METADATA_new.xlsx\")\nquantitative_df = pd.read_excel(\"/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_QUANTITATIVE_METADATA_new.xlsx\")\n\n# Read CSV file\nfnc_df = pd.read_csv(\"/kaggle/input/widsdatathon2025/TRAIN_NEW/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T20:58:46.283445Z","iopub.execute_input":"2025-04-09T20:58:46.283806Z","iopub.status.idle":"2025-04-09T20:59:04.289638Z","shell.execute_reply.started":"2025-04-09T20:58:46.283778Z","shell.execute_reply":"2025-04-09T20:59:04.288326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantitative_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:39:50.689163Z","iopub.execute_input":"2025-04-09T21:39:50.689597Z","iopub.status.idle":"2025-04-09T21:39:50.724501Z","shell.execute_reply.started":"2025-04-09T21:39:50.689566Z","shell.execute_reply":"2025-04-09T21:39:50.723321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantitative_df.set_index('participant_id', inplace=True)\nquantitative_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:39:54.493810Z","iopub.execute_input":"2025-04-09T21:39:54.494167Z","iopub.status.idle":"2025-04-09T21:39:54.509152Z","shell.execute_reply.started":"2025-04-09T21:39:54.494142Z","shell.execute_reply":"2025-04-09T21:39:54.507372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set up the matplotlib figure\nplt.figure(figsize=(20, 15))\n\n# Loop through each column and plot its distribution, dropping NaNs for each column\nfor i, column in enumerate(quantitative_df.columns, 1):\n    plt.subplot(len(quantitative_df.columns) // 3 + 1, 3, i)\n    sns.histplot(quantitative_df[column].dropna(), kde=True, bins=20, color='skyblue', edgecolor='black')\n    plt.title(f'Distribution of {column} (NaNs dropped)')\n    plt.tight_layout()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:02.947044Z","iopub.execute_input":"2025-04-09T21:40:02.947449Z","iopub.status.idle":"2025-04-09T21:40:11.128814Z","shell.execute_reply.started":"2025-04-09T21:40:02.947419Z","shell.execute_reply":"2025-04-09T21:40:11.127750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"desc = pd.read_excel(\"/kaggle/input/widsdatathon2025/Data Dictionary.xlsx\")\ndesc[desc['DataType'] == 'Quantitative']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:16.020593Z","iopub.execute_input":"2025-04-09T21:40:16.020966Z","iopub.status.idle":"2025-04-09T21:40:16.067758Z","shell.execute_reply.started":"2025-04-09T21:40:16.020938Z","shell.execute_reply":"2025-04-09T21:40:16.066637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation matrix excluding 'Id' column\ncorrelation_matrix = quantitative_df.corr()\n\n# Show the top-left part of the matrix\ncorrelation_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:19.329570Z","iopub.execute_input":"2025-04-09T21:40:19.329905Z","iopub.status.idle":"2025-04-09T21:40:19.361071Z","shell.execute_reply.started":"2025-04-09T21:40:19.329880Z","shell.execute_reply":"2025-04-09T21:40:19.359992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'Id' and calculate correlation matrix\nimport numpy as np\n\n# Filter the upper triangle to avoid duplicate pairs and self-correlation\nupper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation > 0.7\nhigh_corr_pairs = upper.stack().reset_index()\nhigh_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\nhigh_corr_pairs = high_corr_pairs[(high_corr_pairs['Correlation'] > 0.7) | (high_corr_pairs['Correlation'] < -0.7)]\n\nprint(high_corr_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:31.002497Z","iopub.execute_input":"2025-04-09T21:40:31.002877Z","iopub.status.idle":"2025-04-09T21:40:31.147532Z","shell.execute_reply.started":"2025-04-09T21:40:31.002848Z","shell.execute_reply":"2025-04-09T21:40:31.146442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values and their percentage\nmissing_values = quantitative_df.isnull().sum()\nmissing_percentage = (missing_values / len(quantitative_df)) * 100\n\n# Combine both the count and percentage into a single DataFrame\nmissing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n\n# Display the result\nmissing_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:34.756454Z","iopub.execute_input":"2025-04-09T21:40:34.756831Z","iopub.status.idle":"2025-04-09T21:40:34.771179Z","shell.execute_reply.started":"2025-04-09T21:40:34.756803Z","shell.execute_reply":"2025-04-09T21:40:34.769969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming 'quantitative_df' is your dataframe\nmri_column = 'MRI_Track_Age_at_Scan'\n\n# Check if for all rows where 'MRI_Track_Age_at_Scan' is missing, no other column is missing\nhypothesis_check = quantitative_df[mri_column].isnull() & quantitative_df.drop(columns=[mri_column]).isnull().any(axis=1)\n\n# If the hypothesis is true, it should be False for all rows (i.e., there should be no rows where MRI is missing but other columns are also missing)\nhypothesis_true = not hypothesis_check.any()\n\n# Output the result\nif hypothesis_true:\n    print(\"The hypothesis is TRUE: All rows where MRI is missing have no other missing values.\")\nelse:\n    print(\"The hypothesis is FALSE: There are rows where MRI is missing and other columns have missing values.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:40:38.753785Z","iopub.execute_input":"2025-04-09T21:40:38.754222Z","iopub.status.idle":"2025-04-09T21:40:38.764238Z","shell.execute_reply.started":"2025-04-09T21:40:38.754167Z","shell.execute_reply":"2025-04-09T21:40:38.763002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Test different values of k\nresults = []\nquantitative_df_copy = quantitative_df.copy()\nquantitative_df_old=quantitative_df.copy()\nfor k in range(1, 21):  # Try k = 1 to 20\n    # Copy the scaled data again for each loop\n    test_data = quantitative_df_copy[quantitative_df_copy['MRI_Track_Age_at_Scan'].notnull()].copy()\n\n    # Randomly mask 10% of values in 'MRI_Track_Age_at_Scan'\n    np.random.seed(42)  # for reproducibility\n    mask_indices = test_data.sample(frac=0.1).index\n    true_values = test_data.loc[mask_indices, 'MRI_Track_Age_at_Scan'].copy()\n    test_data.loc[mask_indices, 'MRI_Track_Age_at_Scan'] = np.nan\n\n    # Apply KNN Imputer with current k\n    knn_imputer = KNNImputer(n_neighbors=k)\n    imputed_array = knn_imputer.fit_transform(test_data)\n\n    # Extract imputed values\n    row_positions = test_data.index.get_indexer(mask_indices)\n    col_position = test_data.columns.get_loc('MRI_Track_Age_at_Scan')\n    imputed_values = imputed_array[row_positions, col_position]\n\n    # Calculate errors\n    mae = mean_absolute_error(true_values, imputed_values)\n    rmse = np.sqrt(mean_squared_error(true_values, imputed_values))\n\n    results.append((k, mae, rmse))\n\n# Convert results to DataFrame for easy analysis\nresults_df = pd.DataFrame(results, columns=['k', 'MAE', 'RMSE'])\nprint(results_df)\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nplt.plot(results_df['k'], results_df['MAE'], marker='o', label='MAE')\nplt.plot(results_df['k'], results_df['RMSE'], marker='x', label='RMSE')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Error')\nplt.title('KNN Imputation Error vs k')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:24.583142Z","iopub.execute_input":"2025-04-09T21:42:24.583529Z","iopub.status.idle":"2025-04-09T21:42:25.291449Z","shell.execute_reply.started":"2025-04-09T21:42:24.583501Z","shell.execute_reply":"2025-04-09T21:42:25.290271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\n# Step 1: Create a copy of the dataframe to avoid modifying the original directly\nquantitative_df_copy = quantitative_df.copy()\nquantitative_df_old=quantitative_df.copy()\n# Step 1: Create a copy of the dataframe to avoid modifying the original directly\nquantitative_df_copy = quantitative_df.copy()\nquantitative_df_old=quantitative_df.copy()\n# Step 2: Scale the data before applying KNN imputation\nscaler = StandardScaler()\n\n# Select columns for scaling, assuming you want to scale all columns except 'MRI_Track_Age_at_Scan'\ncolumns_to_scale = quantitative_df_copy.drop(columns=['MRI_Track_Age_at_Scan']).columns\n\n# Apply scaling\nquantitative_df_copy[columns_to_scale] = scaler.fit_transform(quantitative_df_copy[columns_to_scale])\n\n# Step 3: Initialize KNN Imputer\nknn_imputer = KNNImputer(n_neighbors=9)\n\n# Apply KNN imputation to 'MRI_Track_Age_at_Scan'\nquantitative_df_copy['MRI_Track_Age_at_Scan'] = knn_imputer.fit_transform(quantitative_df_copy[['MRI_Track_Age_at_Scan']])\nquantitative_df['MRI_Track_Age_at_Scan'] =quantitative_df_copy['MRI_Track_Age_at_Scan'] \n# Step 4: After KNN, replace all remaining missing values in the entire dataframe with the median\nquantitative_df.fillna(quantitative_df_copy.median(), inplace=True)\n\n# Step 5: Check the missing values after imputation\nmissing_values_after_imputation = quantitative_df.isnull().sum()\nprint(f\"Missing values after KNN and median imputation:\\n{missing_values_after_imputation}\")\n\n\n# Now 'quantitative_df_temp' has imputed MRI values and median-filled data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:30.058150Z","iopub.execute_input":"2025-04-09T21:42:30.058548Z","iopub.status.idle":"2025-04-09T21:42:30.141810Z","shell.execute_reply.started":"2025-04-09T21:42:30.058521Z","shell.execute_reply":"2025-04-09T21:42:30.140530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantitative_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:33.562768Z","iopub.execute_input":"2025-04-09T21:42:33.563142Z","iopub.status.idle":"2025-04-09T21:42:33.598632Z","shell.execute_reply.started":"2025-04-09T21:42:33.563112Z","shell.execute_reply":"2025-04-09T21:42:33.597257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Set up the matplotlib figure\nplt.figure(figsize=(20, 15))\n\n# Loop through each column and plot its distribution, dropping NaNs for each column\nfor i, column in enumerate(quantitative_df.columns, 1):\n    plt.subplot(len(quantitative_df.columns) // 3 + 1, 3, i)\n    sns.histplot(quantitative_df[column], kde=True, bins=20, color='skyblue', edgecolor='black')\n    plt.title(f'Distribution of {column}')\n    plt.tight_layout()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:36.090853Z","iopub.execute_input":"2025-04-09T21:42:36.091251Z","iopub.status.idle":"2025-04-09T21:42:44.244552Z","shell.execute_reply.started":"2025-04-09T21:42:36.091223Z","shell.execute_reply":"2025-04-09T21:42:44.243488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming quantitative_df is your DataFrame\n# Calculate Q1 (25th percentile), Q3 (75th percentile) and IQR (Interquartile Range)\nQ1 = quantitative_df.quantile(0.25)\nQ3 = quantitative_df.quantile(0.75)\nIQR = Q3 - Q1\n\n# Calculate outlier bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Detect outliers\noutliers = (quantitative_df < lower_bound) | (quantitative_df > upper_bound)\n\n# Count number of outliers and percentage for each column\noutlier_counts = outliers.sum()\noutlier_percentage = (outlier_counts / len(quantitative_df)) * 100\n\n# Create a DataFrame to combine outlier counts and percentages\noutlier_summary = pd.DataFrame({\n    'Outlier Count': outlier_counts,\n    'Outlier Percentage (%)': outlier_percentage\n})\n\n# Print the combined result\nprint(\"Outlier Summary (Count and Percentage):\")\nprint(outlier_summary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:44.245866Z","iopub.execute_input":"2025-04-09T21:42:44.246148Z","iopub.status.idle":"2025-04-09T21:42:44.265373Z","shell.execute_reply.started":"2025-04-09T21:42:44.246126Z","shell.execute_reply":"2025-04-09T21:42:44.264181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy import stats\n\n# Loop through each column in the dataframe\nfor column in quantitative_df.select_dtypes(include=['float64', 'int64']).columns:\n    result = stats.anderson(quantitative_df[column].dropna(), dist='norm')  # Drop NaN values before applying\n    print(f\"\\nAnderson-Darling Test for {column}:\")\n    print(f\"Statistic: {result.statistic}\")\n    for i in range(len(result.critical_values)):\n        print(f\"Critical value at {result.significance_level[i]}% significance level: {result.critical_values[i]}\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorical_df.set_index('participant_id', inplace=True)\n\ncategorical_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:42:58.259323Z","iopub.execute_input":"2025-04-09T21:42:58.259682Z","iopub.status.idle":"2025-04-09T21:42:58.283062Z","shell.execute_reply.started":"2025-04-09T21:42:58.259656Z","shell.execute_reply":"2025-04-09T21:42:58.281851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"desc[desc['DataType'] == 'Categorical ']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:01.962547Z","iopub.execute_input":"2025-04-09T21:43:01.963034Z","iopub.status.idle":"2025-04-09T21:43:01.981304Z","shell.execute_reply.started":"2025-04-09T21:43:01.962991Z","shell.execute_reply":"2025-04-09T21:43:01.980047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation matrix excluding 'Id' column\ncorrelation_matrix = categorical_df.corr()\n\n# Show the top-left part of the matrix\ncorrelation_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:07.152249Z","iopub.execute_input":"2025-04-09T21:43:07.152625Z","iopub.status.idle":"2025-04-09T21:43:07.171704Z","shell.execute_reply.started":"2025-04-09T21:43:07.152598Z","shell.execute_reply":"2025-04-09T21:43:07.170347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'Id' and calculate correlation matrix\nimport numpy as np\n\n# Filter the upper triangle to avoid duplicate pairs and self-correlation\nupper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation > 0.7\nhigh_corr_pairs = upper.stack().reset_index()\nhigh_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\nhigh_corr_pairs = high_corr_pairs[(high_corr_pairs['Correlation'] > 0.7) | (high_corr_pairs['Correlation'] < -0.7)]\n\nprint(high_corr_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:10.397031Z","iopub.execute_input":"2025-04-09T21:43:10.397438Z","iopub.status.idle":"2025-04-09T21:43:10.411061Z","shell.execute_reply.started":"2025-04-09T21:43:10.397409Z","shell.execute_reply":"2025-04-09T21:43:10.409709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values and their percentage\nmissing_values = categorical_df.isnull().sum()\nmissing_percentage = (missing_values / len(categorical_df)) * 100\n\n# Combine both the count and percentage into a single DataFrame\nmissing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n\n# Display the result\nmissing_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:12.899027Z","iopub.execute_input":"2025-04-09T21:43:12.899447Z","iopub.status.idle":"2025-04-09T21:43:12.912867Z","shell.execute_reply.started":"2025-04-09T21:43:12.899415Z","shell.execute_reply":"2025-04-09T21:43:12.911444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming your DataFrame is named df\n\n# Define the columns that should be imputed with -1\ncolumns_to_impute_with_minus_1 = ['Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ']\n\n# Impute the columns with -1\ncategorical_df[columns_to_impute_with_minus_1] = categorical_df[columns_to_impute_with_minus_1].fillna(-1)\n\n# Impute other columns with the mode (most frequent value)\ncolumns_with_missing = categorical_df.columns[categorical_df.isnull().any()]\nfor column in columns_with_missing:\n    if column not in columns_to_impute_with_minus_1:  # Skip the ones already imputed with -1\n        mode_value = categorical_df[column].mode()[0]\n        categorical_df[column] = categorical_df[column].fillna(mode_value)\n\n# Now, df has missing values handled as specified\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:15.630836Z","iopub.execute_input":"2025-04-09T21:43:15.631282Z","iopub.status.idle":"2025-04-09T21:43:15.651420Z","shell.execute_reply.started":"2025-04-09T21:43:15.631252Z","shell.execute_reply":"2025-04-09T21:43:15.650462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 5: Check the missing values after imputation\nmissing_values_after_imputation = categorical_df.isnull().sum()\nprint(f\"Missing values after KNN and median imputation:\\n{missing_values_after_imputation}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:18.313622Z","iopub.execute_input":"2025-04-09T21:43:18.313966Z","iopub.status.idle":"2025-04-09T21:43:18.321494Z","shell.execute_reply.started":"2025-04-09T21:43:18.313941Z","shell.execute_reply":"2025-04-09T21:43:18.320290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the number of unique values for each column\nunique_counts = categorical_df.nunique()\nprint(unique_counts)\n# Calculate the lowest percentage for each column\nlowest_percentage = {}\n\nfor col in categorical_df.columns:\n    # Get value counts and calculate the percentage\n    value_counts = categorical_df[col].value_counts(normalize=True) * 100\n    \n    # Get the lowest percentage\n    lowest_percentage[col] = value_counts.min()\n\n# Print the lowest percentage for each column\nprint(lowest_percentage)\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\n\n# Set up the number of columns and rows in the grid\nn_cols = 3  # Number of columns in the grid\nn_rows = 3  # Number of rows in the grid\n\n# Create a figure with subplots\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 12))\n\n# Flatten axes for easy iteration\naxes = axes.flatten()\n\n# Loop through each column and plot value counts\nfor i, col in enumerate(categorical_df.columns[:9]):  # Limit to first 9 columns\n    categorical_df[col].value_counts().plot(kind='bar', color='skyblue', ax=axes[i])\n    axes[i].set_title(f'Value Counts for {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=45)  # Rotate x-axis labels for better readability\n\n# Adjust layout to avoid overlap\nplt.tight_layout()\nplt.show()\n","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:22.014920Z","iopub.execute_input":"2025-04-09T21:43:22.015284Z","iopub.status.idle":"2025-04-09T21:43:24.015571Z","shell.execute_reply.started":"2025-04-09T21:43:22.015257Z","shell.execute_reply":"2025-04-09T21:43:24.014433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert numeric columns to string (if they are represented as numbers but should be categorical)\ncategorical_df = categorical_df.applymap(str)\n\n# One-hot encode all categorical columns\nencoded_df = pd.get_dummies(categorical_df)\n\n# Display the new DataFrame with one-hot encoded columns\nencoded_df\n\n# Optionally, you can save the new DataFrame to a CSV file\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:43:30.215547Z","iopub.execute_input":"2025-04-09T21:43:30.215903Z","iopub.status.idle":"2025-04-09T21:43:30.266254Z","shell.execute_reply.started":"2025-04-09T21:43:30.215877Z","shell.execute_reply":"2025-04-09T21:43:30.265048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fnc_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:02:36.062812Z","iopub.execute_input":"2025-04-09T21:02:36.063155Z","iopub.status.idle":"2025-04-09T21:02:36.094361Z","shell.execute_reply.started":"2025-04-09T21:02:36.063129Z","shell.execute_reply":"2025-04-09T21:02:36.093298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fnc_df.set_index('participant_id', inplace=True)\n\nfnc_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:02:39.658986Z","iopub.execute_input":"2025-04-09T21:02:39.659420Z","iopub.status.idle":"2025-04-09T21:02:39.690299Z","shell.execute_reply.started":"2025-04-09T21:02:39.659391Z","shell.execute_reply":"2025-04-09T21:02:39.689310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation matrix excluding 'Id' column\ncorrelation_matrix = fnc_df.corr()\n\n# Show the top-left part of the matrix\ncorrelation_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:02:45.439600Z","iopub.execute_input":"2025-04-09T21:02:45.439934Z","iopub.status.idle":"2025-04-09T21:31:37.535927Z","shell.execute_reply.started":"2025-04-09T21:02:45.439909Z","shell.execute_reply":"2025-04-09T21:31:37.534757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'Id' and calculate correlation matrix\nimport numpy as np\n\n# Filter the upper triangle to avoid duplicate pairs and self-correlation\nupper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation > 0.7\nhigh_corr_pairs = upper.stack().reset_index()\nhigh_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\nhigh_corr_pairs = high_corr_pairs[(high_corr_pairs['Correlation'] > 0.7) | (high_corr_pairs['Correlation'] < -0.7)]\n\nprint(high_corr_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:33:11.146702Z","iopub.execute_input":"2025-04-09T21:33:11.147059Z","iopub.status.idle":"2025-04-09T21:33:36.223667Z","shell.execute_reply.started":"2025-04-09T21:33:11.147030Z","shell.execute_reply":"2025-04-09T21:33:36.222037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate missing values and their percentage\nmissing_values = fnc_df.isnull().sum()\nmissing_percentage = (missing_values / len(fnc_df)) * 100\n\n# Combine both the count and percentage into a single DataFrame\nmissing_data = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n\n# Display the result\nmissing_data\nmissing_data.sum()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:34:47.117863Z","iopub.execute_input":"2025-04-09T21:34:47.118273Z","iopub.status.idle":"2025-04-09T21:34:47.202684Z","shell.execute_reply.started":"2025-04-09T21:34:47.118236Z","shell.execute_reply":"2025-04-09T21:34:47.201562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming quantitative_df is your DataFrame\n# Calculate Q1 (25th percentile), Q3 (75th percentile) and IQR (Interquartile Range)\nQ1 = fnc_df.quantile(0.25)\nQ3 = fnc_df.quantile(0.75)\nIQR = Q3 - Q1\n\n# Calculate outlier bounds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Detect outliers\noutliers = (fnc_df < lower_bound) | (fnc_df > upper_bound)\n\n# Count number of outliers and percentage for each column\noutlier_counts = outliers.sum()\noutlier_percentage = (outlier_counts / len(fnc_df)) * 100\n\n# Create a DataFrame to combine outlier counts and percentages\noutlier_summary = pd.DataFrame({\n    'Outlier Count': outlier_counts,\n    'Outlier Percentage (%)': outlier_percentage\n})\n\n# Print the combined result\nprint(\"Outlier Summary (Count and Percentage):\")\nprint(outlier_summary)\nprint(outlier_summary.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:39:07.614164Z","iopub.execute_input":"2025-04-09T21:39:07.614607Z","iopub.status.idle":"2025-04-09T21:39:09.009931Z","shell.execute_reply.started":"2025-04-09T21:39:07.614576Z","shell.execute_reply":"2025-04-09T21:39:09.008835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine quantitative_df, fnc_df, and encoded_df along columns (axis=1)\ncombined_df = pd.concat([quantitative_df, fnc_df, encoded_df], axis=1)\n\n# Check the first few rows of the combined DataFrame\ncombined_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:45:49.198887Z","iopub.execute_input":"2025-04-09T21:45:49.199288Z","iopub.status.idle":"2025-04-09T21:45:49.324471Z","shell.execute_reply.started":"2025-04-09T21:45:49.199260Z","shell.execute_reply":"2025-04-09T21:45:49.323454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute correlation matrix excluding 'Id' column\ncorrelation_matrix_t = combined_df.corr()\n\n# Show the top-left part of the matrix\ncorrelation_matrix_t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T21:46:35.265538Z","iopub.execute_input":"2025-04-09T21:46:35.265881Z","iopub.status.idle":"2025-04-09T22:15:44.014496Z","shell.execute_reply.started":"2025-04-09T21:46:35.265856Z","shell.execute_reply":"2025-04-09T22:15:44.012686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop 'Id' and calculate correlation matrix\nimport numpy as np\n\n# Filter the upper triangle to avoid duplicate pairs and self-correlation\nupper = correlation_matrix_t.where(np.triu(np.ones(correlation_matrix_t.shape), k=1).astype(bool))\n\n# Find features with correlation > 0.7\nhigh_corr_pairs = upper.stack().reset_index()\nhigh_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\nhigh_corr_pairs = high_corr_pairs[(high_corr_pairs['Correlation'] >= 0.75) | (high_corr_pairs['Correlation'] <= -0.75)]\n\nprint(high_corr_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T22:42:54.941726Z","iopub.execute_input":"2025-04-09T22:42:54.942111Z","iopub.status.idle":"2025-04-09T22:43:23.379361Z","shell.execute_reply.started":"2025-04-09T22:42:54.942059Z","shell.execute_reply":"2025-04-09T22:43:23.378108Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"solutions_df","metadata":{},"outputs":[],"execution_count":null}]}